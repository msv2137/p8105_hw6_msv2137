---
title: "HW 6"
author: "Malvika Venkataraman"
date: "12/4/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
```

# Problem 1

## Load and Clean Data
```{r}
birthweight = read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
    babysex = factor(babysex),
    frace = factor(frace),
    malform = factor(malform),
    mrace = factor(mrace)
  )

str(birthweight)
#View(birthweight)
```
The are no missing values in this dataset.

## Model

I'll propose for this problem using backward stepwise selection, with the step() function. 
```{r}
model_init <- lm(bwt ~., data = birthweight)
step(model_init, direction = "backward")
```
This model selection approach starts with all predictors in the model. It then removes the the least significant predictor. It repeats this process until all non-significant predictors have been removed.

My final model includes the following predictors:

* baby sex
* baby head circumference
* baby length
* mother’s weight at delivery
* family monthly income
* gestational age
* mother’s height
* mother’s race
* number of lives births prior to this pregnancy
* mother’s pre-pregnancy weight
* average number of cigarettes smoked per day during pregnancy

```{r}
model_back = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)
```

## Plot of Model Residuals vs. Fitted Values.

```{r}
birthweight %>%
  add_residuals(model_back) %>%
  add_predictions(model_back) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point(alpha = .3) +
  labs(title = "Residuals vs. Fitted Values")
```

## Model Comparison

I'll compare my model to two others: 

* Main Effect Model: A model using length at birth and gestational age as predictors 
* Interaction Model: One using head circumference, length, sex, and all interactions between these

```{r, warning=FALSE}
cv_df = crossv_mc(birthweight, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble),
    backwards_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    mod_main_effect = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod_interaction = map(train, ~lm(bwt ~ bhead + blength + babysex 
                                     + bhead * blength 
                                     + bhead * babysex 
                                     + blength * babysex 
                                     + bhead * blength * babysex, data = .x)), 
    rmse_backwards_mod = map2_dbl(backwards_mod, test, ~rmse(model = .x, data = .y)),
    rmse_mod_main_effect = map2_dbl(mod_main_effect, test, ~rmse(model = .x, data = .y)),
    rmse_mod_interaction = map2_dbl(mod_interaction, test, ~rmse(model = .x, data = .y)))
  
```

```{r}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(title = "RMSE for Cross-Validated Models", x = "Model Type", y = "RMSE")
```

The plot shows that the model made using backward stepwise selection has the lowest RMSE, and therefore is the best performing of the three, in terms of predicting baby weight. The interaction model has a slightly greater RMSE than the proposed model, however the inclusion of interaction terms would make it harder to interpret than the other models. The main effect model had the highest RMSE.

# Problem 2

## Load Data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

## Simple Linear Regression Model

```{r}
#regression model
lm(tmax ~ tmin, data = weather_df) %>%
  broom::tidy() %>%
  knitr::kable(digits = 3)
```

## Bootstrapped Model

```{r}
set.seed(1)

boot_sample = function(df) {
  sample_frac(weather_df, replace = T)
}

boot_strap =
  tibble(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

boot_results =
  boot_strap %>%
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    result_1 = map(models, broom::glance),
    result_2 = map(models, broom::tidy)) %>%
  select(strap_number, result_1, result_2) %>%
  unnest(c(result_1, result_2), names_repair = "unique")
```

## Estimates

### R Squared Estimate
```{r}
#r squared
boot_results %>%
  summarize(mean_r_sq = mean(r.squared))
```
The mean R-squared estimate over all bootstrapped samples is `r boot_results %>% summarize(mean_r_sq = mean(r.squared))`

### log(β̂ 0∗β̂ 1) Estimate
```{r}
# log(B0*B1)
bootstrap_log =
  boot_results %>%
  select(strap_number, term, estimate) %>%
  pivot_wider(
    names_from = term,
    values_from = estimate) %>%
  rename("B0" = "(Intercept)",
         "B1" = "tmin") %>%
  mutate(
    log_var = log(B0*B1)
  )
```

```{r}
bootstrap_log %>%
  summarize(mean_log_var = mean(log_var))
```
The mean log(β̂ 0∗β̂ ) estimate over all bootstrapped samples is `r bootstrap_log %>% summarize(mean_log_var = mean(log_var))`

## Plot of the Distribution of the Two Estimates

```{r}
#plot of r squared
boot_results %>%
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(title = "Distribution of R-Squared")
```

The plot is centered around .91, and it ranges approximately from .88 to .94. The distribution looks approximately Normal.

```{r}
# plot of log(B0*B1)

bootstrap_log %>%
  ggplot(aes(x = log_var)) +
  geom_density() +
  labs(title = "Distribution of log(B_hat_0*B_hat_1)")
```

The plot is centered around 2.01, and it ranges approximately from 1.9 to 2.09. The distribution looks approximately Normal.

## Confidence Intervals

### R Squared
```{r}
# r squared 95% CI
boot_results %>%
  summarize(
    ci_lower = quantile(r.squared, .025),
    ci_upper = quantile(r.squared, .975)
  )
```
We are 95% confident that the true mean R-squared value lies between .89 and .93.

### log(β̂ 0∗β̂ 1)
```{r}
# log(B0*B1) 95% CI

bootstrap_log %>%
  summarize(
    ci_lower = quantile(log_var, .025),
    ci_upper = quantile(log_var, .975)
  )
```
We are 95% confident that the true mean log(β̂0 * β̂1) lies between 1.96 and 2.06.
